# -*- coding: utf-8 -*-
"""Scraping_details_page.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1psEg68E01gAfsZD3xNd_aM0vF7v3XwPB
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import json

"""# Details page for tunisiapromo"""

csv_file = 'tunisiapromo.csv'

apartments_df = pd.read_csv(csv_file)
print(apartments_df)

apartments_df['description_detailed'] = "N/A"
apartments_df['details'] = "N/A"
apartments_df['caracteristiques'] = "N/A"

for index, row in apartments_df.iterrows():
    try:
        print(f"Scraping details for: {row['headline']}...")

        # Access the detail link from the dataset
        details_link = row['details_link']

        # Fetch the detail page
        response = requests.get(details_link)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract additional details (adjust selectors as needed)
        # Extract description
        description_detailed = "N/A"  # Default value if no description is found
        details = "N/A"
        caracteristiques = "N/A"

        clear_divs = soup.find_all('div', class_='clear')

        for div in clear_divs:
            p_tag = div.find('p')
            details_div = div.find('div', class_='leftColumn')
            caracteristiques_div = div.find('div', class_='rightColumn')
            if p_tag:
                description_detailed = p_tag.get_text(separator=" ", strip=True)
            if details_div:
                details = details_div.get_text(separator=" ", strip=True)
            listing_item = div.find('span', class_='listing-item')
            if listing_item and "Caractéristiques" in listing_item.get_text(strip=True):
                caracteristiques = div.get_text(separator=" ", strip=True).replace("Caractéristiques :", "").strip()
                break

        # Update the DataFrame with the new data
        apartments_df.at[index, 'description_detailed'] = description_detailed
        apartments_df.at[index, 'details'] = details
        apartments_df.at[index, 'caracteristiques'] = caracteristiques

        # Print or store the scraped data
        print(f"Description: {description_detailed}")
        print(f"Details: {details}")
        print(f"Caracteristiques: {caracteristiques}")

    except Exception as e:
        print(f"Error scraping details for {row['title']}: {e}")

apartments_df.to_csv('updated_apartments_details.csv', index=False)

"""# Details page for Remax"""

csv_file = 'Remax.csv'

apartments_df['surface_constructible'] = "N/A"
apartments_df['places_parking'] = "N/A"
apartments_df['nombre_etages'] = "N/A"
apartments_df['etage'] = "N/A"
apartments_df['description'] = "N/A"
apartments_df['caracteristiques'] = "N/A"

for index, row in apartments_df.iterrows():
    try:
        print(f"Scraping details for: {row['title']}...")

        # Access the detail link from the dataset
        details_link = row['details_link']

        # Fetch the detail page
        response = requests.get(details_link)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Initialize variables
        surface_constructible = "N/A"
        places_parking = "N/A"
        nombre_etages = "N/A"
        etage = "N/A"
        description = "N/A"
        caracteristiques = []

        # Extract surface constructible
        surface_div = soup.find('div', class_='attributes-data-row')
        if surface_div:
            surface_constructible_tag = surface_div.find('div', {'data-original-title': lambda x: x and "Surface constructible" in x})
            if surface_constructible_tag:
                surface_constructible = surface_constructible_tag.find_next('span', dir="ltr").text.strip()

        # Extract places de parking
        parking_tag = surface_div.find('div', {'data-original-title': lambda x: x and "Places de parking" in x})
        if parking_tag:
            places_parking = parking_tag.find_next('span', dir="ltr").text.strip()

        # Extract nombre d'étages
        nombre_etages_tag = soup.find('div', {'data-original-title': lambda x: x and "Nombre d'étages" in x})
        if nombre_etages_tag:
            nombre_etages = nombre_etages_tag.find_next('span', dir="ltr").text.strip()

        # Extract étage
        etage_tag = soup.find('div', {'data-original-title': lambda x: x and "Etage" in x})
        if etage_tag:
            etage = etage_tag.find_next('span', dir="ltr").text.strip()

        # Extract description
        description_div = soup.find('div', id="ListingFullLeft_ctl01_DescriptionDivShort")
        if description_div:
            description = description_div.text.strip()

        # Extract caractéristiques
        features_container = soup.find('div', class_='features-container')
        if features_container:
            feature_items = features_container.find_all('span', class_='feature-item')
            caracteristiques = [item.text.strip() for item in feature_items]

        # Update the DataFrame with the new data
        apartments_df.at[index, 'surface_constructible'] = surface_constructible
        apartments_df.at[index, 'places_parking'] = places_parking
        apartments_df.at[index, 'nombre_etages'] = nombre_etages
        apartments_df.at[index, 'etage'] = etage
        apartments_df.at[index, 'description'] = description
        apartments_df.at[index, 'caracteristiques'] = ", ".join(caracteristiques)

        # Print the scraped data
        print(f"Surface Constructible: {surface_constructible}")
        print(f"Places Parking: {places_parking}")
        print(f"Nombre Etages: {nombre_etages}")
        print(f"Etage: {etage}")
        print(f"Description: {description}")
        print(f"Caracteristiques: {', '.join(caracteristiques)}")

    except Exception as e:
        print(f"Error scraping details for {row['title']}: {e}")

# Save the updated DataFrame to CSV
apartments_df.to_csv('updated_remax_apartments_details.csv', index=False)

"""# Details page for 9annas"""

# Regex patterns for extracting data (case-insensitive matching)
patterns = {
    "type_de_bien": r"Type de bien:\s*([^\n]+)",
    "etat": r"Etat:\s*([^\n]+)",
    "caracteristiques": r"Caractéristiques:\s*([^\n]+)",
    "type_de_transaction": r"Type de transaction:\s*([^\n]+)",
    "superficie": r"Superficie:\s*([^\n]+)",
    "salles_de_bains": r"Salles de bains:\s*(\d+)",
    "chambres": r"Chambres:\s*(\d+)",
    "etage_du_bien": r"Étage du bien:\s*([^\n]+)",
    "etat_du_bien": r"Etat du bien:\s*([^\n]+)",
}

# Regex for extracting specific fields from "Caractéristiques"
superficie_from_caracteristiques = r"(\d+\s*m²)"
chambres_from_caracteristiques = r"(\d+)\s*Chambres?"
salles_de_bains_from_caracteristiques = r"(\d+)\s*Salles?\s*de\s*bains?"

# Function to extract and delete details from the description
def clean_description(description):
    if not isinstance(description, str):
        return description, {}

    extracted_data = {}
    extracted_from_caracteristiques = []

    # Extract fields using regex with re.IGNORECASE
    for field, pattern in patterns.items():
        match = re.search(pattern, description, re.IGNORECASE)
        if match:
            extracted_data[field] = match.group(1)
            description = re.sub(pattern, '', description, flags=re.IGNORECASE)  # Remove matched part

    # If "superficie" was not found in the description, extract it from "Caractéristiques"
    if "superficie" not in extracted_data:
        match = re.search(superficie_from_caracteristiques, extracted_data.get("caracteristiques", ""), re.IGNORECASE)
        if match:
            extracted_data["superficie"] = match.group(1)
            extracted_from_caracteristiques.append("superficie")

    # If "Chambres" was not found in the description, extract it from "Caractéristiques"
    if "chambres" not in extracted_data:
        match = re.search(chambres_from_caracteristiques, extracted_data.get("caracteristiques", ""), re.IGNORECASE)
        if match:
            extracted_data["chambres"] = match.group(1)
            extracted_from_caracteristiques.append("chambres")

    # If "Salles de bains" was not found in the description, extract it from "Caractéristiques"
    if "salles_de_bains" not in extracted_data:
        match = re.search(salles_de_bains_from_caracteristiques, extracted_data.get("caracteristiques", ""), re.IGNORECASE)
        if match:
            extracted_data["salles_de_bains"] = match.group(1)
            extracted_from_caracteristiques.append("salles_de_bains")

    # Clean the extracted fields from "Caractéristiques"
    if "caracteristiques" in extracted_data:
        for field in extracted_from_caracteristiques:
            pattern_to_remove = eval(f"{field}_from_caracteristiques")
            extracted_data["caracteristiques"] = re.sub(pattern_to_remove, '', extracted_data["caracteristiques"], flags=re.IGNORECASE).strip()

        # Remove trailing, leading, and redundant commas from "Caractéristiques"
        extracted_data["caracteristiques"] = re.sub(r",\s*,*", ',', extracted_data["caracteristiques"])
        extracted_data["caracteristiques"] = extracted_data["caracteristiques"].strip(', ')

        # Additional step: remove any ", ," occurrences caused by gaps
        extracted_data["caracteristiques"] = re.sub(r",\s*,", ',', extracted_data["caracteristiques"])

    # Return cleaned description and extracted data
    return description.strip(), extracted_data

# Read the CSV into a pandas DataFrame
csv_file = '9annas2.csv'
df = pd.read_csv(csv_file)

# Assuming the CSV has a 'description' column
for index, row in df.iterrows():
    cleaned_description, extracted_data = clean_description(row['description'])
    df.at[index, 'description'] = cleaned_description  # Update the description in the DataFrame

    # Add the extracted fields to the DataFrame
    for field, value in extracted_data.items():
        df.at[index, field] = value

# Write the updated DataFrame back to the CSV
df.to_csv('updated_9annas_data.csv', index=False)

# Optionally, print the updated DataFrame to verify
print(df.head())

"""# Splitting details data from tunisiapromo

"""

# Define the fields to extract
fields = [
    "Nombre de pièce(s)",
    "Nombre de salle(s) de bain",
    "Nombre de salle(s) d'eau",
    "Année de Construction",
    "Surface habitable",
    "Surface Totale",
    "Nombre de place de Voiture",
    "Numéro / Nombre d'étages",
    "Orientation"
]

# Function to extract values for all fields
def extract_details(details):
    if not isinstance(details, str):  # Handle non-string or missing values
        return {field: None for field in fields}

    result = {}
    for i, field in enumerate(fields):
        # Define the regex pattern for the current field and the next one
        if i < len(fields) - 1:
            next_field = fields[i + 1]
            pattern = rf"{re.escape(field)}\s*:\s*(.*?)\s*(?={re.escape(next_field)}\s*:)"
        else:
            # Last field: Capture everything after its colon
            pattern = rf"{re.escape(field)}\s*:\s*(.*)"

        # Extract the match
        match = re.search(pattern, details, re.DOTALL)
        result[field] = match.group(1).strip() if match and match.group(1).strip() else None

    return result

# Load the CSV
csv_file = 'tunisiapromo detailed only with price.csv'
df = pd.read_csv(csv_file)

# Apply the extraction function to the 'details' column
extracted_data = df['details'].apply(extract_details)

# Convert the extracted data (list of dicts) to a DataFrame
extracted_df = pd.DataFrame(extracted_data.tolist())

# Concatenate the original DataFrame with the extracted fields
final_df = pd.concat([df, extracted_df], axis=1)

# Save the final DataFrame to a new CSV
final_df.to_csv('processed_details.csv', index=False)

# Optionally preview the result
print(final_df.head())

"""# Deleting redundant records from 9annas between the 4 states"""


# bab bhar/ montfleury/ Le bardo/ sidi el béchir/ les berges du lac/ el omrane supérieur/ cité intilaka/ cité ettahrir
# cité jardins/ cité ezzouhour/ bab ....

# enkhilette el manzah/ cité ennasr/ el manar
city = 'el omrane supérieur'

# Load the CSV file
file = '9annas ariana.csv'

# Read the CSV into a DataFrame
ariana_df = pd.read_csv(file)

# Normalize the relevant columns to lowercase for case-insensitive comparison
ariana_df['preview-location-secondary-info'] = ariana_df['preview-location-secondary-info'].str.lower()
ariana_df['description'] = ariana_df['description'].str.lower()

# Filter out rows containing the city in either column
filtered_ariana_df = ariana_df[
    ~((ariana_df['preview-location-secondary-info'].str.contains(city, na=False)) |
      (ariana_df['description'].str.contains(city, na=False)))
]

# Save the filtered DataFrame back to the CSV file
filtered_ariana_df.to_csv(file, index=False)

# Output the number of records after removal
print(f"Updated record count in '{file}': {len(filtered_ariana_df)}")
print("Records containing the city have been removed.")

"""# Check redundancy of records of 9annas using preview-image href(unique)"""


# Load the CSV file
csv_file = 'tunisiapromo detailed only with price.csv'
unique_attribute = 'details_link'
df = pd.read_csv(csv_file)

# Count occurrences of each value
duplicates = df[unique_attribute].value_counts()

# Identify redundant records (where count > 1)
redundant_records = duplicates[duplicates > 1]

# Total number of unique redundant entries
number_of_redundant_entries = len(redundant_records)

# Total number of redundant occurrences
total_redundant_occurrences = redundant_records.sum() - number_of_redundant_entries

# Output the redundant preview-image href values
print(total_redundant_occurrences)
print(redundant_records.index.tolist())

"""# Remove villa/ immeuble records"""


# Load the CSV file into a DataFrame
csv_file = 'full.csv'
df = pd.read_csv(csv_file)

# Count the number of records where 'villa' is in the 'description' or 'title' columns (case insensitive)
records_to_delete = df[
    df['description'].str.contains(r'\bimmeuble\b', case=False, na=False) |
    df['title'].str.contains(r'\bimmeuble\b', case=False, na=False)
]

# Calculate the number of records to delete
num_records_deleted = len(records_to_delete)

# Remove rows where 'villa' is in 'description' or 'title' columns (case insensitive)
df_cleaned = df[
    ~df['description'].str.contains(r'\bimmeuble\b', case=False, na=False) &
    ~df['title'].str.contains(r'\bimmeuble\b', case=False, na=False)
]

# Save the cleaned DataFrame back to a new CSV file
df_cleaned.to_csv('cleaned_full.csv', index=False)

# Output the number of records deleted
print(f"Number of records deleted: {num_records_deleted}")

# Optionally, print the cleaned DataFrame to verify
print(df_cleaned.head())

"""# Splitting state city in tunisiapromo dataset"""


# Load the CSV file into a DataFrame
csv_file = 'location.csv'
df = pd.read_csv(csv_file)

# Define the list of states
states = ['Manouba', 'Tunis', 'Ben arous', 'Ariana']

# Function to extract state and location
def extract_state_and_location(location):
    # Split the string by ' / ' (if it exists)
    parts = location.split(' / ', 1)

    # Check if the first part is a valid state
    if parts[0] in states:
        state = parts[0]
        rest_of_location = parts[1] if len(parts) > 1 else ''
    else:
        state = ''
        rest_of_location = location

    return state, rest_of_location

# Apply the function to the 'location' column
df[['state', 'location']] = df['location'].apply(lambda x: pd.Series(extract_state_and_location(x)))

# Save the cleaned DataFrame to a new CSV file
df.to_csv('final_location.csv', index=False)

# Optionally, print the cleaned DataFrame to verify
print(df.head())

"""# New Section"""

# Input JSON file
input_json_file = 'FULLTreated.json'

# Output CSV file
output_csv_file = 'data.csv'

# Load JSON file
with open(input_json_file, 'r') as file:
    data = json.load(file)

# Convert JSON data to a pandas DataFrame
df = pd.json_normalize(data)

# Save DataFrame to a CSV file
df.to_csv(output_csv_file, index=False)

print(f"CSV file '{output_csv_file}' created successfully!")


# Input CSV file
input_csv_file = 'split.csv'

# Output JSON file
output_json_file = 'data.json'

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(input_csv_file)

# Convert the DataFrame to a JSON file
df.to_json(output_json_file, orient='records', lines=False, indent=4)

print(f"JSON file '{output_json_file}' created successfully!")

"""# Extra codes"""



data = [
    {
        "id": 24468724,
        "title": "Appartement S2 à vendre à riadh landalous",
        "description": "JUST ON TIME IMMO met à votre disposition un appartement S2 pour la vente à riadh landalous ariana. L’appartement se compose comme suit un salon spacieux avec balcon  deux chambres à coucher  une SDE  une SDB  Une cuisine equipée avec sechoir  climatisation split  chauffage central place de parking abris  titre individuel  pour plus d’information 29667106 ou 29667274 ou 29668094\n\nType de bien: Appartement\nEtat: Nouveau\nCaractéristiques: 86 m², 2 Pièces, 2 Chambres, 1 Salle de bain, Climatisation, Chauffage central",
        "price": 250000,
        "categoryId": 3,
        "location": "Citée Ennouzha, Ariana",
    },
    {
        "id": 24468726,
        "title": "Appartement S+2 à vendre à riadh landalous",
        "description": "JUST ON TIME IMMO met à votre disposition un appartement S+2 pour la vente à riadh landalous ariana.\r\nL’appartement se compose comme suit:\r\nun salon spacieux avec balcon \r\ndeux chambres à coucher \r\nune SDE \r\nune SDB \r\nUne cuisine equipée avec sechoir \r\nclimatisation split \r\nchauffage central\r\nplace de parking abris \r\n titre individuel \r\npour plus d’information : 29667106 ou 29667274 ou 29668094\n\nType de transaction: À Vendre\nSuperficie: 86 m²\nSalles de bains: 0\nChambres: 2",
        "price": 250000,
        "categoryId": 3,
        "location": "Riadh Andalous, Ariana",
    },
    {
        "id": 24468621,
        "title": "À vendre d'un S3 spacieux à la nouvelle Soukra",
        "description": "À Vendre Chez MARSA Négoce : Appartement S3 à La Nouvelle Soukra \n\nNous vous proposons à la vente ce superbe appartement S3, niché dans une résidence calme et sécurisée à La Nouvelle Soukra. Situé au 1er étage avec ascenseur, cet appartement lumineux et spacieux offre un cadre de vie confortable et pratique.\n\nCaractéristiques de l'appartement :\n\nSalon : Avec balcon\n\nSalle d’eau pour invités\n\nCuisine équipée\n\nSuite parentale\n\nDeux chambres à coucher avec dressing\n\nSalle de bain commune\n\nCet appartement est doté de nombreuses commodités modernes :\n\nChauffage central\n\nClimatisation\n\nMoustiquaires dans toutes les fenêtres\n\nPlace de parking au sous-sol\n\nProfitez d'un cadre de vie agréable, proche de toutes les commodités et dans un quartier paisible.\n\nSuperficie : 152 m²\n\nPrix de vente : 445.000 MDT\n\nPour plus d'informations ou pour organiser une visite, contactez-nous dès aujourd'hui !\n\nType de bien: Appartement\nEtat: Nouveau\nEtat du bien: 1-5 ans\nÉtage du bien: 1er\nOrientation: Ouest\nType du sol: Marbre\nCaractéristiques: 152 m², 4 Pièces, 3 Chambres, 3 Salles de bains, Terrasse, Garage, Ascenseur, Concierge, Antenne parabolique, Climatisation, Chauffage central, Sécurité, Double vitrage, Porte blindée, Cuisine équipée",
        "price": 445000,
        "categoryId": 3,
        "location": "La Soukra, Ariana",
    }
]

# Regex patterns for extracting data
patterns = {
    "type_de_bien": r"Type de bien:\s*([^\n]+)",
    "etat": r"Etat:\s*([^\n]+)",
    "caracteristiques": r"Caractéristiques:\s*([^\n]+)",
    "type_de_transaction": r"Type de transaction:\s*([^\n]+)",
    "superficie": r"Superficie:\s*([^\n]+)",
    "salles_de_bains": r"Salles de bains:\s*(\d+)",
    "chambres": r"Chambres:\s*(\d+)",
    "etage_du_bien": r"Étage du bien:\s*([^\n]+)",
    "etat_du_bien": r"Etat du bien:\s*([^\n]+)"
}

# Function to extract and delete details from the description
def clean_description(description):
    extracted_data = {}

    # Extract data using regex
    for field, pattern in patterns.items():
        match = re.search(pattern, description)
        if match:
            extracted_data[field] = match.group(1)
            description = description.replace(match.group(0), '')  # Remove the matched part from description

    # Return cleaned description and extracted data
    return description.strip(), extracted_data

# Process the data
for item in data:
    cleaned_description, extracted_data = clean_description(item['description'])
    item['description'] = cleaned_description  # Update the description with the cleaned version
    item.update(extracted_data)  # Add extracted data fields to the item

# Print the updated data
for item in data:
    print(item)